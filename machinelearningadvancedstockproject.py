# -*- coding: utf-8 -*-
"""MachineLearningAdvancedStockProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11lFRlV1pZKK0yXajMkpxIz0HFvVagbuE
"""

!pip install optuna xgboost lightgbm tensorflow --quiet
import yfinance as yf
import pandas as pd
import numpy as np
import concurrent.futures
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.model_selection import TimeSeriesSplit
from sklearn.ensemble import RandomForestRegressor, VotingRegressor
from xgboost import XGBRegressor
import lightgbm as lgb
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Bidirectional
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.metrics import mean_absolute_error, mean_squared_error
import joblib
import optuna
from statsmodels.tsa.arima.model import ARIMA
import warnings
warnings.filterwarnings("ignore")

tickers = ["AAPL", "TSLA", "GOOGL", "MSFT", "AMZN", "NVDA", "META", "JPM", "NFLX", "AMD", "IBM"]
def fetch_stock_data(ticker):
    stock = yf.Ticker(ticker)
    data = stock.history(period="10y", interval="1d")
    if data.empty: return None
    data = data[['Open', 'High', 'Low', 'Close', 'Volume']].reset_index()
    data["Ticker"] = ticker
    return data

def fetch_multiple_stocks(tickers):
    all_data = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        results = executor.map(fetch_stock_data, tickers)
        for result in results:
            if result is not None: all_data.append(result)
    if not all_data: return None
    df = pd.concat(all_data, ignore_index=True).dropna()

    df['MA7'] = df.groupby('Ticker')['Close'].rolling(7).mean().reset_index(0,drop=True)
    df['MA21'] = df.groupby('Ticker')['Close'].rolling(21).mean().reset_index(0,drop=True)
    df['MA50'] = df.groupby('Ticker')['Close'].rolling(50).mean().reset_index(0,drop=True)
    df['MA200'] = df.groupby('Ticker')['Close'].rolling(200).mean().reset_index(0,drop=True)

    df['Daily_Return'] = df.groupby('Ticker')['Close'].pct_change()
    df['Volatility_7'] = df.groupby('Ticker')['Daily_Return'].rolling(7).std().reset_index(0,drop=True)
    df['Volatility_21'] = df.groupby('Ticker')['Daily_Return'].rolling(21).std().reset_index(0,drop=True)
    df['RSI'] = compute_rsi(df.groupby('Ticker')['Close'], 14)
    df['Price_Momentum'] = df.groupby('Ticker')['Close'].pct_change(5)
    df['Volume_SMA'] = df.groupby('Ticker')['Volume'].rolling(20).mean().reset_index(0,drop=True)
    return df.dropna()

def compute_rsi(prices, window=14):
    delta = prices.diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
    rs = gain / loss
    return 100 - (100 / (1 + rs))

stock_data = fetch_multiple_stocks(tickers)
stock_data.to_csv("advanced_stock_data.csv", index=False)

df = pd.read_csv("advanced_stock_data.csv")
df['Date'] = pd.to_datetime(df['Date'])
df = df.sort_values(['Ticker', 'Date'])

features = ['Open', 'High', 'Low', 'Close', 'Volume', 'MA7', 'MA21', 'MA50', 'MA200',
           'Daily_Return', 'Volatility_7', 'Volatility_21', 'RSI', 'Price_Momentum', 'Volume_SMA']

scaler_features = StandardScaler()
scaler_target = StandardScaler()

X = scaler_features.fit_transform(df[features].fillna(0))
y = scaler_target.fit_transform(df['Close'].values.reshape(-1,1)).ravel()

split_idx = int(0.8 * len(X))
X_train, X_test = X[:split_idx], X[split_idx:]
y_train, y_test = y[:split_idx], y[split_idx:]

np.save("X_train.npy", X_train); np.save("X_test.npy", X_test)
np.save("y_train.npy", y_train); np.save("y_test.npy", y_test)
print(f"âœ“ Train: {X_train.shape}, Test: {X_test.shape}")

def objective_xgb(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 100, 500),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'subsample': trial.suggest_float('subsample', 0.7, 1.0)
    }
    model = XGBRegressor(**params, random_state=42)
    model.fit(X_train, y_train)
    return mean_absolute_error(y_test, model.predict(X_test))

def objective_lgb(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 100, 500),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'num_leaves': trial.suggest_int('num_leaves', 20, 100)
    }
    model = lgb.LGBMRegressor(**params, random_state=42, verbose=-1)
    model.fit(X_train, y_train)
    return mean_absolute_error(y_test, model.predict(X_test))

study_xgb = optuna.create_study(direction='minimize')
study_xgb.optimize(objective_xgb, n_trials=20)

study_lgb = optuna.create_study(direction='minimize')
study_lgb.optimize(objective_lgb, n_trials=20)

xgb_best = XGBRegressor(**study_xgb.best_params, random_state=42)
xgb_best.fit(X_train, y_train)

lgb_best = lgb.LGBMRegressor(**study_lgb.best_params, random_state=42, verbose=-1)
lgb_best.fit(X_train, y_train)

X_train_lstm = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])
X_test_lstm = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])

lstm_advanced = Sequential([
    Bidirectional(LSTM(64, return_sequences=True), input_shape=(1, X_train.shape[1])),
    BatchNormalization(),
    Dropout(0.3),
    Bidirectional(LSTM(32)),
    Dropout(0.3),
    Dense(16, activation='relu'),
    Dense(1)
])
lstm_advanced.compile(optimizer='adam', loss='mse')

callbacks = [EarlyStopping(patience=10), ReduceLROnPlateau(patience=5)]
lstm_advanced.fit(X_train_lstm, y_train, epochs=100, batch_size=32,
                  callbacks=callbacks, verbose=0)

ensemble = VotingRegressor([
    ('xgb', xgb_best),
    ('lgb', lgb_best)
])
ensemble.fit(X_train, y_train)

predictions = {
    'XGBoost': xgb_best.predict(X_test),
    'LightGBM': lgb_best.predict(X_test),
    'LSTM': lstm_advanced.predict(X_test_lstm).flatten(),
    'Ensemble': ensemble.predict(X_test)
}

best_mae = float('inf')
best_model = None
for name, pred in predictions.items():
    mae = mean_absolute_error(y_test, pred)
    mse = mean_squared_error(y_test, pred)
    print(f"{name:<12} MAE: {mae:.4f} | MSE: {mse:.4f}")
    if mae < best_mae:
        best_mae = mae
        best_model = name

print(f"\n BEST: {best_model} (MAE: {best_mae:.4f})")

joblib.dump(scaler_features, "scaler_features.pkl")
joblib.dump(scaler_target, "scaler_target.pkl")

joblib.dump(xgb_best, "production_xgb.pkl")
joblib.dump(lgb_best, "production_lgb.pkl")
joblib.dump(ensemble, "production_ensemble.pkl")
lstm_advanced.save("production_lstm.keras")

joblib.dump({
    'best_model': best_model,
    'xgb_params': study_xgb.best_params,
    'lgb_params': study_lgb.best_params,
    'performance': {k: mean_absolute_error(y_test, v) for k,v in predictions.items()}
}, "production_info.pkl")